<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Web Crawler</title>
    <link>https://rc-web-crawler.github.io/</link>
    <description>Recent content on Web Crawler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://rc-web-crawler.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.1 Web at a glance</title>
      <link>https://rc-web-crawler.github.io/1st_home/1_web_at_glance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/1st_home/1_web_at_glance/</guid>
      <description>1. Web at a glance  1.1 Overview of a webpage  1.1.1 HTML 1.1.2 CSS 1.1.3 JavaScript      Is web crawling and scraping legal?
 Octoparse : If you’re doing web crawling for your own purposes, then it is legal as it falls under fair use doctrine. The complications start if you want to use scraped data for others, especially commercial purposes. As long as you are not crawling at a disruptive rate and the source is public you should be fine.</description>
    </item>
    
    <item>
      <title>HTML elements &amp; Tree Structure</title>
      <link>https://rc-web-crawler.github.io/1st_home/2_html_elements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/1st_home/2_html_elements/</guid>
      <description>2. HTML elements &amp;amp; Tree Structure  2.1 HTML elements 2.2 HTML Attributes 2.3 Tree structure of HTML document  2.3.1 Relationship of Nodes      2. HTML elements &amp;amp; Tree Structure 2.1 HTML elements An HTML element is defined by a start tag, some content, and an end tag. The HTML element is everything from the start tag to the end tag:
&amp;lt;tagname&amp;gt;Content goes here...&amp;lt;/tagname&amp;gt;</description>
    </item>
    
    <item>
      <title>CSS selector &amp; Xpath</title>
      <link>https://rc-web-crawler.github.io/1st_home/3_css/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/1st_home/3_css/</guid>
      <description>3.1 Introduction CSS, Cascading Style Sheets, is a style sheet language used for describing the presentation of a document written in a markup language such as HTML. And CSS selectors are used to select the content you want to style. Selectors are the part of CSS rule set. CSS selectors select HTML elements according to its id, class, type, attribute etc.
XPath, XML Path Language, is a query language for selecting nodes from an XML document.</description>
    </item>
    
    <item>
      <title>Demo: Web Scraping with R/Python</title>
      <link>https://rc-web-crawler.github.io/1st_home/4_demo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/1st_home/4_demo/</guid>
      <description>Pre-prepare: Include packages that we need pythonRfrom bs4 import BeautifulSoup # beautifulsoup4 import requests import pandas as pd import numpy as np import re library(rvest) library(dplyr) Within Novartis, we need proxy to get into some websites.
Pre-prepare: Interact with the website pythonRurl = &amp;#34;https://www.lexjansen.com&amp;#34; proxyDict = {&amp;#39;https&amp;#39;: xxx} # Interact with data via a REST API # Returns a &amp;lt;response&amp;gt; object r = requests.</description>
    </item>
    
    <item>
      <title>Keyword Analysis</title>
      <link>https://rc-web-crawler.github.io/2nd_home/5_keyword/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/2nd_home/5_keyword/</guid>
      <description>Purpose =======
In this section, we will use text mining methods to derive information from the text of keywords. We explore the frequency, coverage, and the relationship between keywords, therefore identifying keywords which are important for our further analysis and model building.
Pre-prepare: install R packages and import data ===============================================
library(knitr)library(readxl)library(tidyverse)library(tidytext)library(igraph)library(ggraph)library(textstem)library(RSQLite)library(plotly)data &amp;lt;- read_excel(&amp;quot;~/Downloads//paper_keyword.xlsx&amp;quot;) Data transformation ===================
Keyword cleaning  We transform the data into a tibble (tibbles are a modern take on data frames) and add the row number with the column name ‘document’.</description>
    </item>
    
    <item>
      <title>Content Analysis</title>
      <link>https://rc-web-crawler.github.io/2nd_home/6_content/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/2nd_home/6_content/</guid>
      <description>Title Analysis: Keyword Prediction Although most of the articles have keywords with them, there are still quite a lot of articles without keywords. Meanwhile, some articles may have complicated keywords, which could also increase the difficulty of analysis. To solve this problem, we can try to predict the keywords with title analysis techniques.
In the following parts, the keyword prediction using the results from previous keyword analysis will be demonstrated.</description>
    </item>
    
    <item>
      <title>eCRF Index Shiny App</title>
      <link>https://rc-web-crawler.github.io/2nd_home/7_shiny/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/2nd_home/7_shiny/</guid>
      <description>Instruction Users could upload the eCRF pdf file using new template to get the index for eCRF forms and map with SDTM domains. According to alpha test results, the coverage rate is 70% ~ 85% and the accuracy rate is 85% ~ 100%.
Limitation As expected, mapping results for efficacy-related domains, like QS, ZW, are low in accuracy and coverage, since their eCRF forms are always study specific forms
Shiny App </description>
    </item>
    
    <item>
      <title>credits</title>
      <link>https://rc-web-crawler.github.io/credits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rc-web-crawler.github.io/credits/</guid>
      <description>References  Beautiful soup4 documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Lexjansen Paper website: https://www.lexjansen.com XPATH cheat sheet: https://devhints.io/xpath CSS selector cheat sheet: https://frontend30.com/css-selectors-cheatsheet/  </description>
    </item>
    
  </channel>
</rss>
