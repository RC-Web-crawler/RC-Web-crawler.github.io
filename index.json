[
{
	"uri": "https://rc-web-crawler.github.io/1_web_at_glance/",
	"title": "Web at a glance",
	"tags": [],
	"description": "",
	"content": "爬虫入门教程 该教程总共分为4部分内容：\n 了解网页 节点及节点之间的关系 节点的抓取与选择 用R与Python对网站进行简单的爬取   爬虫入门教程  1. 了解网页  1.1 认识网页结构  1.1.1 HTML 1.1.2 CSS 1.1.3 JavaScript   1.2 关于爬虫的合法性   2. 节点及节点的关系    1. 了解网页 1.1 认识网页结构 网页一般由三部分组成，分别是 HTML（超文本标记语言）、CSS（层叠样式表）和 JavaScript（活动脚本语言）。\n1.1.1 HTML 首先让我们来看一个简单的网页：webpage_raw\n这个网页背后的源代码是：\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;爬虫小组的测试页\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;欢迎来到爬虫小组的网页！\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;这是一个简单的测试页\u0026lt;/p\u0026gt;\r\u0026lt;h2\u0026gt;团队成员与重要日期\u0026lt;/h2\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2020年6月10日\u0026lt;/b\u0026gt; — 于昊永加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2020年8月20日\u0026lt;/b\u0026gt; — 杨帆加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021年6月1日\u0026lt;/b\u0026gt; — 王崧人加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021年7月1日\u0026lt;/b\u0026gt; — 韩梦岳加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021年7月1日\u0026lt;/b\u0026gt; — 滕书言加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;2021年7月1日\u0026lt;/b\u0026gt; — 姚艾文加入诺华的日子\u0026lt;/p\u0026gt;\r\u0026lt;h2\u0026gt;更多信息\u0026lt;/h2\u0026gt;\r\u0026lt;p\u0026gt;Gitlab: \u0026lt;a href=\u0026quot;http://gitlabce.apps.dit-prdocp.novartis.net/YUHAY/web-crawler-do.git\u0026quot;\u0026gt;Web Craler DO\u0026lt;/a\u0026gt;\r\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rHTML 是整个网页的结构，相当于整个网站的框架。带“＜”、“＞”符号的都是属于 HTML 的标签，并且标签都是成对出现的。\n常见的标签如下：\n\u0026lt;!DOCTYPE html\u0026gt; 声明为 HTML5 文档\r\u0026lt;head\u0026gt; 包含了文档的元（meta）数据，如 \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; 定义网页编码格式为 utf-8\r\u0026lt;html\u0026gt;..\u0026lt;/html\u0026gt; 表示标记中间的元素是网页\r\u0026lt;body\u0026gt;..\u0026lt;/body\u0026gt; 表示用户可见的内容\r\u0026lt;div\u0026gt;..\u0026lt;/div\u0026gt; 表示框架\r\u0026lt;p\u0026gt;..\u0026lt;/p\u0026gt; 表示段落\r\u0026lt;li\u0026gt;..\u0026lt;/li\u0026gt;表示列表\r\u0026lt;img\u0026gt;..\u0026lt;/img\u0026gt;表示图片\r\u0026lt;h1\u0026gt;..\u0026lt;/h1\u0026gt;表示标题\r\u0026lt;a href=\u0026quot;\u0026quot;\u0026gt;..\u0026lt;/a\u0026gt;表示超链接\r1.1.2 CSS CSS 表示样式，\u0026lt;style type=\u0026quot;text/css\u0026quot;\u0026gt;表示下面引用一个 CSS，在 CSS 中定义了外观。\n我们来看一下加入下面的CSS之后，网页会变成什么样子: webpage_css\n \u0026lt;style type=\u0026quot;text/css\u0026quot;\u0026gt;\r@import url('https://fonts.googleapis.com/css2?family=ZCOOL+KuaiLe\u0026amp;display=swap');\rbody {background-image: url('https://www.statnews.com/wp-content/uploads/2018/05/Novartis-1.jpg');\rfont-family: 'ZCOOL KuaiLe';}\rh1 {color: blanchedalmond;font-size: 50px;}\rp {color:tomato; font-size: 25px;}\rh2 {color: yellow;font-size: 30px;}\ra {color:turquoise;}\r\u0026lt;/style\u0026gt;\r1.1.3 JavaScript JavaScript 表示功能。交互的内容和各种特效都在 JavaScript 中，JavaScript 描述了网站中的各种功能。让我们一起来看一个的例子：webpage_js\n如果用人体来比喻，HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里。CSS 是人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等。JavaScript 表示人的技能，例如跳舞、唱歌或者演奏乐器等。\n1.2 关于爬虫的合法性 几乎每一个网站都有一个名为 robots.txt 的文档，当然也有部分网站没有设定 robots.txt。对于没有设定 robots.txt 的网站可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。如果网站有 robots.txt 文档，就要判断是否有禁止访客获取的数据。\n2. 节点及节点的关系    a b     askldjlaskjd asiodiasdaskjd    i need to highlight ==so funny==\n:joy: :christmas_tree:\n"
},
{
	"uri": "https://rc-web-crawler.github.io/3_css/",
	"title": "CSS selectorts &amp; Xpath",
	"tags": [],
	"description": "",
	"content": "========\nsongren 12/27/2021 Introduction XPath is a query language for selecting nodes from an XML document. In addition, XPath may be used to compute values from the content of an XML document. XPath was defined by the World Wide Web Consortium. CSS selectors are used to select the content you want to style. Selectors are the part of CSS rule set. CSS selectors select HTML elements according to its id, class, type, attribute etc.(CSS selector: https://www.w3school.com.cn/cssref/css_selectors.asp, Xpath: https://www.w3school.com.cn/xpath/index.asp)\nSummary CSS selector Summary Syntax Syntax\nExample\nResult\n.class\n.intro\nSelects all elements with class=“intro”\n.class1.class2\n.name1.name2\nSelects all elements with both name1 and name2 set within its class attribute\n.class1 .class2\n.name1 .name2\nSelects all elements with name2 that is a descendant of an element with name1\n#id\n#firstname\nSelects the element with id=“firstname”\nasterisk\nasterisk\nSelects all elements\nelement\np\nSelects all elements\nelement.class\np.intro\nSelects all elements with class=“intro”\nelement,element\ndiv, p\nSelects all elements and all elements\nelement element\ndiv p\nSelects all elements inside elements\nelement\u0026gt;element\ndiv \u0026gt; p\nSelects all elements where the parent is a element\nelement+element\ndiv + p\nSelects the first element that is placed immediately after elements\nelement1~element2\np ~ ul\nSelects every element that is preceded by a element\n[attribute]\n[target]\nSelects all elements with a target attribute\n[attribute=value]\n[target=_blank]\nSelects all elements with target=\u0026quot;_blank\u0026quot;\n[attribute~=value]\n[title~=flower]\nSelects all elements with a title attribute containing the word “flower”\n[attribute|=value]\n[lang|=en]\nSelects all elements with a lang attribute value equal to “en” or starting with “en-”\n[attribute^=value]\na[href^=“https”]\nSelects every element whose href attribute value begins with “https”\n[attribute$=value]\na[href$=“.pdf”]\nSelects every element whose href attribute value ends with “.pdf”\n[attribute*=value]\na[href*=“w3schools”]\nSelects every element whose href attribute value contains the substring “w3schools”\nExpression Expression\nExample\nResult\n:active\na:active\nSelects the active link\n::after\np::after\nInsert something after the content of each element\n::before\np::before\nInsert something before the content of each element\n:checked\ninput:checked\nSelects every checked element\n:default\ninput:default\nSelects the default element\n:disabled\ninput:disabled\nSelects every disabled element\n:empty\np:empty\nSelects every element that has no children (including text nodes)\n:enabled\ninput:enabled\nSelects every enabled element\n:first-child\np:first-child\nSelects every element that is the first child of its parent\n::first-letter\np::first-letter\nSelects the first letter of every element\n::first-line\np::first-line\nSelects the first line of every element\n:first-of-type\np:first-of-type\nSelects every element that is the first element of its parent\n:focus\ninput:focus\nSelects the input element which has focus\n:fullscreen\n:fullscreen\nSelects the element that is in full-screen mode\n:hover\na:hover\nSelects links on mouse over\n:in-range\ninput:in-range\nSelects input elements with a value within a specified range\n:indeterminate\ninput:indeterminate\nSelects input elements that are in an indeterminate state\n:invalid\ninput:invalid\nSelects all input elements with an invalid value\n:lang(language)\np:lang(it)\nSelects every element with a lang attribute equal to “it” (Italian)\n:last-child\np:last-child\nSelects every element that is the last child of its parent\n:last-of-type\np:last-of-type\nSelects every element that is the last element of its parent\n:link\na:link\nSelects all unvisited links\n::marker\n::marker\nSelects the markers of list items\n:not(selector)\n:not(p)\nSelects every element that is not a element\n:nth-child(n)\np:nth-child(2)\nSelects every element that is the second child of its parent\n:nth-last-child(n)\np:nth-last-child(2)\nSelects every element that is the second child of its parent, counting from the last child\n:nth-last-of-type(n)\np:nth-last-of-type(2)\nSelects every element that is the second element of its parent, counting from the last child\n:nth-of-type(n)\np:nth-of-type(2)\nSelects every element that is the second element of its parent\n:only-of-type\np:only-of-type\nSelects every element that is the only element of its parent\n:only-child\np:only-child\nSelects every element that is the only child of its parent\n:optional\ninput:optional\nSelects input elements with no “required” attribute\n:out-of-range\ninput:out-of-range\nSelects input elements with a value outside a specified range\n::placeholder\ninput::placeholder\nSelects input elements with the “placeholder” attribute specified\n:read-only\ninput:read-only\nSelects input elements with the “readonly” attribute specified\n:read-write\ninput:read-write\nSelects input elements with the “readonly” attribute NOT specified\n:required\ninput:required\nSelects input elements with the “required” attribute specified\n:root\n:root\nSelects the document’s root element\n::selection\n::selection\nSelects the portion of an element that is selected by a user\n:target\n#news:target\nSelects the current active #news element (clicked on a URL containing that anchor name)\n:valid\ninput:valid\nSelects all input elements with a valid value\n:visited\na:visited\nSelects all visited links\nXPATH Summary Syntax Expression\nDescription\nExample\nResult\nnodename\nSelects all nodes with the name “nodename”\nbookstore\nSelects all nodes with the name “bookstore”\n/\nSelects from the root node\n/bookstore\nSelects the root element bookstore\n//\nSelects nodes in the document from the current node that match the selection no matter where they are\nbookstore/book\nSelects all book elements that are children of bookstore\n.\nSelects the current node\n//book\nSelects all book elements no matter where they are in the document\n..\nSelects the parent of the current node\nbookstore//book\nSelects all book elements that are descendant of the bookstore element, no matter where they are under the bookstore element\n@\nSelects attributes\n//@lang\nSelects all attributes that are named lang\n*\nMatches any element node\n/bookstore/*\nSelects all the child element nodes of the bookstore element\n@*\nMatches any attribute node\n//*\nSelects all elements in the document\nnode()\nMatches any node of any kind\n//title[@*]\nSelects all title elements which have at least one attribute of any kind\nNote: If the path starts with a slash ( / ) it always represents an absolute path to an element!\nAxes Axesname\nDescription\nExample\nResult\nancestor\nSelects all ancestors (parent, grandparent, etc.) of the current node\nancestor::book\nSelects all book nodes that are ancestor of the current node\nancestor-or-self\nSelects all ancestors (parent, grandparent, etc.) of the current node and the current node itself\nattribute\nSelects all attributes of the current node\nattribute::*\nSelects all attributes of the current node\nchild\nSelects all children of the current node\nchild::*/child::price\nSelects all price grandchildren of the current node\ndescendant\nSelects all descendants (children, grandchildren, etc.) of the current node\ndescendant::book\nSelects all book descendants of the current node\ndescendant-or-self\nSelects all descendants (children, grandchildren, etc.) of the current node and the current node itself\nfollowing\nSelects everything in the document after the closing tag of the current node\nfollowing::text()\nSelects all text node that are everything after the current node\nfollowing-sibling\nSelects all siblings after the current node\nfollowing-sibling::node()\nSelects all siblings after the current node\nnamespace\nSelects all namespace nodes of the current node\nparent\nSelects the parent of the current node\nparent::book\nSelects all book nodes that are parent of the current node\npreceding\nSelects all nodes that appear before the current node in the document, except ancestors, attribute nodes and namespace nodes\npreceding::price\nSelects all price nodes that appear before the current node\npreceding-sibling\nSelects all siblings before the current node\npreceding-sibling::book[price\u0026gt;50.0]\nSelects book nodes with price\u0026gt;50 from siblings before the current node\nself\nSelects the current node\nself::*\nSelects all in the current node\nOperator Operator\nDescription\nExample\n|\nComputes two node-sets\n//book | //cd\n+\nAddition\n6 + 4\n-\nSubtraction\n6 - 4\n*\nMultiplication\n6 * 4\ndiv\nDivision\n8 div 4\n=\nEqual\nprice=9.80\n!=\nNot equal\nprice!=9.80\n\u0026lt;\nLess than\nprice\u0026lt;9.80\n\u0026lt;=\nLess than or equal to\nprice\u0026lt;=9.80\n\u0026gt;\nGreater than\nprice\u0026gt;9.80\n\u0026gt;=\nGreater than or equal to\nprice\u0026gt;=9.80\nor\nor\nprice=9.80 or price=9.70\nand\nand\nprice\u0026gt;9.00 and price\u0026lt;9.90\nmod\nModulus (division remainder)\n5 mod 2\nExamples HTML Structure ## \u0026lt;!DOCTYPE html\u0026gt;\r## \u0026lt;html\u0026gt;\r## \u0026lt;head\u0026gt;\r## \u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=UTF-8\u0026quot;\u0026gt;\r## \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\r## \u0026lt;title\u0026gt;Webscrap introduction\u0026lt;/title\u0026gt;\r## \u0026lt;/head\u0026gt;\r## \u0026lt;body\u0026gt;\r## \u0026lt;div class=\u0026quot;class1\u0026quot; id=\u0026quot;div1\u0026quot;\u0026gt;\r## \u0026lt;p class=\u0026quot;class3\u0026quot;\u0026gt;\r## Welcome 2022!\r## \u0026lt;t class=\u0026quot;text\u0026quot;\u0026gt;\r## Beat COVID-19!\r## \u0026lt;/t\u0026gt;\u0026lt;/p\u0026gt;\r## \u0026lt;/div\u0026gt;\r## \u0026lt;div class=\u0026quot;class2\u0026quot; id=\u0026quot;div2\u0026quot;\u0026gt;\r## \u0026lt;p class=\u0026quot;class3\u0026quot; href=\u0026quot;https://en.wikipedia.org/wiki/Cat\u0026quot;\u0026gt;\r## Happy Spring Festival!\r## \u0026lt;t class=\u0026quot;text\u0026quot;\u0026gt; Dumpling YYDS! \u0026lt;/t\u0026gt;\u0026lt;price\u0026gt;0.00\u0026lt;/price\u0026gt;\u0026lt;/p\u0026gt;\r## \u0026lt;/div\u0026gt;\r## \u0026lt;div class=\u0026quot;class1 class2\u0026quot; id=\u0026quot;div3\u0026quot;\u0026gt;\r## \u0026lt;p class=\u0026quot;class3\u0026quot; href=\u0026quot;https://en.wikipedia.org/wiki/Cat\u0026quot;\u0026gt;\r## Happy Valentine's Day!\r## \u0026lt;t class=\u0026quot;text\u0026quot;\u0026gt; Let's romantic! \u0026lt;/t\u0026gt;\u0026lt;price\u0026gt;100.0\u0026lt;/price\u0026gt;\u0026lt;/p\u0026gt;\r## \u0026lt;p class=\u0026quot;class4\u0026quot; href=\u0026quot;https://en.wikipedia.org/wiki/Cat\u0026quot;\u0026gt;\r## Happy Lantern Festival!\r## \u0026lt;t class=\u0026quot;text\u0026quot;\u0026gt; Sweet Dumpling YYDS! \u0026lt;/t\u0026gt;\u0026lt;price\u0026gt;0.0\u0026lt;/price\u0026gt;\u0026lt;/p\u0026gt;\r## \u0026lt;/div\u0026gt;\r## \u0026lt;/body\u0026gt;\r## \u0026lt;/html\u0026gt;\r##  Example 1 css selector check_element_text(html = html, css = \u0026quot;head \u0026gt; title\u0026quot;)\r## [1] \u0026quot;Webscrap introduction\u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;/html/head/title\u0026quot;)\r## [1] \u0026quot;Webscrap introduction\u0026quot;\r Example 2 css selector check_element_text(html = html, css = \u0026quot;body p\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot; ## [2] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00\u0026quot; ## [3] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0\u0026quot; ## [4] \u0026quot; Happy Lantern Festival! Sweet Dumpling YYDS! 0.0\u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//body//p\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot; ## [2] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00\u0026quot; ## [3] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0\u0026quot; ## [4] \u0026quot; Happy Lantern Festival! Sweet Dumpling YYDS! 0.0\u0026quot;\r Example 3 css selector check_element_text(html = html, css = \u0026quot;t\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//t\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r Example 4 css selector check_element_text(html = html, css = \u0026quot;div:first-of-type\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot;\rcheck_element_text(html = html, css = \u0026quot;div:nth-of-type(2)\u0026quot;)\r## [1] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00 \u0026quot;\rcheck_element_text(html = html, css = \u0026quot;div:last-of-type\u0026quot;)\r## [1] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0 Happy Lantern Festival! Sweet Dumpling YYDS! 0.0 \u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//div[position()=1]\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot;\rcheck_element_text(html = html, xpath = \u0026quot;//div[2]\u0026quot;)\r## [1] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00 \u0026quot;\rcheck_element_text(html = html, xpath = \u0026quot;//div[last()]\u0026quot;)\r## [1] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0 Happy Lantern Festival! Sweet Dumpling YYDS! 0.0 \u0026quot;\r Example 5 css selector check_element_text(html = html, css = \u0026quot;div#div2\u0026quot;)\r## [1] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00 \u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//div[@id='div2']\u0026quot;)\r## [1] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00 \u0026quot;\r Example 6 css selector check_element_text(html = html, css = \u0026quot;t.text\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//t[@class='text']\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r Example 7 css selector check_element_text(html = html, css = \u0026quot;*.class3\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot; ## [2] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00\u0026quot; ## [3] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0\u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//*[@class='class3']\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot; ## [2] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00\u0026quot; ## [3] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0\u0026quot;\r Example 8 css selector check_element_text(html = html, css = \u0026quot;t, price\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot;0.00\u0026quot; ## [4] \u0026quot; Let's romantic! \u0026quot; \u0026quot;100.0\u0026quot; \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r## [7] \u0026quot;0.0\u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//t | //price\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot;0.00\u0026quot; ## [4] \u0026quot; Let's romantic! \u0026quot; \u0026quot;100.0\u0026quot; \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r## [7] \u0026quot;0.0\u0026quot;\r Example 9 css selector check_element_text(html = html, css = \u0026quot;div t\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\r xpath check_element_text(html = html, xpath = \u0026quot;//div/descendant::t\u0026quot;)\r## [1] \u0026quot; Beat COVID-19! \u0026quot; \u0026quot; Dumpling YYDS! \u0026quot; \u0026quot; Let's romantic! \u0026quot; ## [4] \u0026quot; Sweet Dumpling YYDS! \u0026quot;\rcheck_element_text(html = html, xpath = \u0026quot;//t/ancestor::div\u0026quot;)\r## [1] \u0026quot; Welcome 2022! Beat COVID-19! \u0026quot; ## [2] \u0026quot; Happy Spring Festival! Dumpling YYDS! 0.00 \u0026quot; ## [3] \u0026quot; Happy Valentine's Day! Let's romantic! 100.0 Happy Lantern Festival! Sweet Dumpling YYDS! 0.0 \u0026quot;\r Example 10 css selector ## Not avaiable\r xpath check_element_text(html = html, xpath = \u0026quot;//p[price\u0026gt;50.0]/t\u0026quot;)\r## [1] \u0026quot; Let's romantic! \u0026quot;\r // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $(\u0026lsquo;tr.odd\u0026rsquo;).parent(\u0026lsquo;tbody\u0026rsquo;).parent(\u0026lsquo;table\u0026rsquo;).addClass(\u0026lsquo;table table-condensed\u0026rsquo;); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\u0026ldquo;TOC\u0026rdquo;); }); $(document).ready(function () { $('.tabset-dropdown \u0026gt; .nav-tabs \u0026gt; li').click(function () { $(this).parent().toggleClass(\u0026lsquo;nav-tabs-open\u0026rsquo;); }); }); $(document).ready(function () { // temporarily add toc-ignore selector to headers for the consistency with Pandoc $('.unlisted.unnumbered').addClass(\u0026lsquo;toc-ignore\u0026rsquo;) // move toc-ignore selectors from section div to header $(\u0026lsquo;div.section.toc-ignore\u0026rsquo;) .removeClass(\u0026lsquo;toc-ignore\u0026rsquo;) .children(\u0026lsquo;h1,h2,h3,h4,h5\u0026rsquo;).addClass(\u0026lsquo;toc-ignore\u0026rsquo;); // establish options var options = { selectors: \u0026ldquo;h1,h2,h3\u0026rdquo;, theme: \u0026ldquo;bootstrap3\u0026rdquo;, context: \u0026lsquo;.toc-content\u0026rsquo;, hashGenerator: function (text) { return text.replace(/[.\\\\/?\u0026amp;!#\u0026lt;\u0026gt;]/g, \u0026lsquo;').replace(/\\s/g, \u0026lsquo;_'); }, ignoreSelector: \u0026ldquo;.toc-ignore\u0026rdquo;, scrollTo: 0 }; options.showAndHide = true; options.smoothScroll = true; // tocify var toc = $(\u0026quot;#TOC\u0026quot;).tocify(options).data(\u0026ldquo;toc-tocify\u0026rdquo;); }); (function () { var script = document.createElement(\u0026ldquo;script\u0026rdquo;); script.type = \u0026ldquo;text/javascript\u0026rdquo;; script.src = \u0026ldquo;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026rdquo;; document.getElementsByTagName(\u0026ldquo;head\u0026rdquo;)[0].appendChild(script); })();\n"
},
{
	"uri": "https://rc-web-crawler.github.io/4_demo/",
	"title": "Demo: Web Scraping with R/Python",
	"tags": [],
	"description": "",
	"content": " Pre-prepare: Include packages that we need python\rR\r\rfrom bs4 import BeautifulSoup # beautifulsoup4 import requests import pandas as pd import numpy as np import re \r\rlibrary() \r\r\r\rWithin Novartis, we need proxy to get into some websites.\n\rPre-prepare: Interact with the website python\rR\r\rurl = \u0026#34;https://www.lexjansen.com\u0026#34; proxyDict = {\u0026#39;https\u0026#39;: \u0026#39;http://na-useh-proxy.na.novartis.net:2011\u0026#39;} # Interact with data via a REST API # Returns a \u0026lt;response\u0026gt; object r = requests.get(url, proxies=proxyDict, verify=False) # for css selector soup = BeautifulSoup(r.text, \u0026#39;lxml\u0026#39;) # for XPath tree = html.fromstring(r.content) \r\rlibrary() \r\r\r\rDifferent parsers:\n \u0026lsquo;lxml\u0026rsquo;, \u0026lsquo;html.parser\u0026rsquo;, \u0026lsquo;xml\u0026rsquo;\u0026hellip; If it is a perfectly-formed HTML document, small differences between parsers. If the document is not perfectly-formed, different parsers give different results.  \r\rHTML file\r\r\r\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!--[if lt IE 7]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9 lt-ie8 lt-ie7\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if IE 7]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9 lt-ie8\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if IE 8]\u0026gt; \u0026lt;html class=\u0026#34;no-js lt-ie9\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;!--[if gt IE 8]\u0026gt; \u0026lt;html class=\u0026#34;no-js\u0026#34; lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;IE=edge\u0026#34; http-equiv=\u0026#34;X-UA-Compatible\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;https://fonts.googleapis.com/css?family=Special+Elite\u0026amp;amp;v1\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;-gdlTJkYvyNRwnnZSHmM94zhCJNRmCwBfdNWk5u2yII\u0026#34; name=\u0026#34;google-site-verification\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;c2f6e02546ad6b1a\u0026#34; name=\u0026#34;yandex-verification\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;telephone=no\u0026#34; name=\u0026#34;format-detection\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;none\u0026#34; name=\u0026#34;msapplication-config\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;SAS Proceedings and more: Fortune Records, Dave Marsh 1001, ...\u0026#34; name=\u0026#34;description\u0026#34;/\u0026gt; \u0026lt;meta content=\u0026#34;SAS Proceedings, Lex Jansen, Fortune Records, The Heart of Rock and Soul, Dave Marsh 1001\u0026#34; name=\u0026#34;KeyWords\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;SAS Proceedings and more\u0026lt;/title\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt;{\u0026#34;@context\u0026#34;:\u0026#34;https:\\/\\/schema.org\u0026#34;,\u0026#34;@type\u0026#34;:\u0026#34;WebSite\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/www.lexjansen.com\\/\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Lex Jansen\u0026#39;s Homepage\u0026#34;}\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt;{\u0026#34;@context\u0026#34;:\u0026#34;https:\\/\\/schema.org\u0026#34;,\u0026#34;@type\u0026#34;:\u0026#34;Person\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;https:\\/\\/www.lexjansen.com\\/\u0026#34;, \u0026#34;sameAs\u0026#34;:[\u0026#34;https:\\/\\/instagram.com\\/lex.jansen\\/\u0026#34;,\u0026#34;https:\\/\\/www.evernote.com\\/pub\\/lexjansen\\/public\u0026#34;,\u0026#34;https:\\/\\/www.linkedin.com\\/in\\/lexjansen\u0026#34;,\u0026#34;https:\\/\\/plus.google.com\\/u\\/0\\/+LexJansen\u0026#34;,\u0026#34;https:\\/\\/www.youtube.com\\/user\\/lexjansen\u0026#34;,\u0026#34;https:\\/\\/twitter.com\\/lexjansen\u0026#34;],\u0026#34;name\u0026#34;:\u0026#34;Lex jansen\u0026#34;}\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026#34;favicon.ico\u0026#34; rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;favicon.ico\u0026#34; rel=\u0026#34;shortcut icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;/apple-touch-icon.png\u0026#34; rel=\u0026#34;apple-touch-icon\u0026#34;/\u0026gt; \u0026lt;link href=\u0026#34;/apple-touch-icon-57x57.png\u0026#34; rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;57x57\u0026#34;/\u0026gt; ... \u0026lt;br/\u0026gt; Copyright © 1999-2022 Lex Jansen. All rights reserved.\u0026lt;br/\u0026gt; SAS is a registered trademark of SAS Institute Inc. SAS, Statistical Analysis System, and all other SAS Institute Inc. product or service names are registered trademarks or trademarks of SAS Institute Inc. in the USA and other countries. ® indicates USA registration. \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;https://www.google-analytics.com/urchin.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; _uacct = \u0026#34;UA-132232-1\u0026#34;; urchinTracker();\u0026lt;/script\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; data-goatcounter=\u0026#34;https://lexjansen.goatcounter.com/count\u0026#34; src=\u0026#34;//gc.zgo.at/count.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var gaJsHost = ((\u0026#34;https:\u0026#34; == document.location.protocol) ? \u0026#34;https://ssl.\u0026#34; : \u0026#34;https://www.\u0026#34;); document.write(unescape(\u0026#34;%3Cscript src=\u0026#39;\u0026#34; + gaJsHost + \u0026#34;google-analytics.com/ga.js\u0026#39; type=\u0026#39;text/javascript\u0026#39;%3E%3C/script%3E\u0026#34;)); \u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var pageTracker = _gat._getTracker(\u0026#34;UA-132232-1\u0026#34;); pageTracker._trackPageview(); \u0026lt;/script\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \r\r Structure graph TB;\rA[Title, Link, Author, Keyword, Pages, Size]\rB[Conference name, Conference place, Conference time]\rC[Section name, Best paper flag]\rF[Final dataset]\rA -- F\rB -- F C -- F\rsubgraph g1 [Paper information]\rA\rend\rsubgraph g2[Conference information]\rB\rend\rsubgraph g3[Paper attributes]\rC\rend\r\rOne example that we use    Paper information      Title A Case Study of Mining Social Media Data for Disaster Relief: Hurricane Irma   Link https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2018/2695-2018.pdf   Author Bogdan Gadidov, Linh Le   Keyword Text Mining Topic Modeling Time Series   Pages 11   Sizes 660 kb       Conference information      Conference name SAS Global Forum 2018   Conference place Denver, Colorado   Conference time April 8-11, 2018       Paper attributes      Section name Breakout   Best paper flag     1. Homepage -\u0026gt; Get the SUGI url graph LR;\rA[\"div id='sasproceedings'\"] -- B[ul]\rB -- C[li];\rC -- D[\"div class='pf'\"];\rD -- E[div];\rE -- F[span] -- G[a];\rstyle G fill:#f3d7d3;\r\r\u0026lt;a href=\u0026quot;/sugi\u0026quot;\u0026gt;SUGI / SAS Global Forum\u0026lt;/a\u0026gt; papers (1976-2021)\npython\rR\r\r# CSS selector soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] \u0026gt; ul \u0026gt; li \u0026gt; div \u0026gt; div \u0026gt; span \u0026gt; a\u0026#34;) soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] a\u0026#34;) # XPath tree.xpath(\u0026#39;//div[@id =\u0026#34;sasproceedings\u0026#34;]/ul/li/div/div/span/a/text()\u0026#39;) tree.xpath(\u0026#39;//div[@id =\u0026#34;sasproceedings\u0026#34;]/descendant::a/text()\u0026#39;) # Get SUGI URL href = soup.select(\u0026#34;div[id = \u0026#39;sasproceedings\u0026#39;] a\u0026#34;)[5].get(\u0026#39;href\u0026#39;) sugi_url = url + href \r\rlibrary() \r\r\r\r2. SUGI / SAS Global Forum -\u0026gt; Get conference information \rgraph LR;\rA[\"ul class='conferences'\"] -- B[\"li class='conference'\"]\rB -- C[a];\rB -- D[span];\rB -- E[span];\rstyle C fill:#f3d7d3;\rstyle D fill:#f3d7d3;\rstyle E fill:#f3d7d3;\r\r\u0026lt;a href=\u0026quot;../cgi-bin/xsl_transform.php?x=sgf2018\u0026quot;\u0026gt;SAS Global Forum 2018\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt;April 8-11, 2018\u0026lt;/span\u0026gt; \u0026lt;span\u0026gt;Denver, Colorado\u0026lt;/span\u0026gt;\npython\rR\r\rr_sugi = requests.get(sugi_url, proxies=proxyDict, verify=False) soup2 = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) tree2 = html.fromstring(r_sugi.content) # SUGI Forun 2018 url soup2.select(\u0026#39;li a\u0026#39;)[3].get(\u0026#39;href\u0026#39;) sugi_2018_url = url + soup2.select(\u0026#39;li a\u0026#39;)[3].get(\u0026#39;href\u0026#39;)[2:] # 1. conference name # CSS selector soup2.select(\u0026#34;li a\u0026#34;)[3].text # XPath tree2.xpath(\u0026#39;//li/a/text()\u0026#39;)[3] # 2. conference time # CSS selector soup2.select(\u0026#34;li[class = \u0026#39;conference\u0026#39;]\u0026#34;)[3].select(\u0026#34;span\u0026#34;)[0].text # XPath: first \u0026lt;span\u0026gt; element under \u0026lt;li\u0026gt; tree2.xpath(\u0026#39;//li/span[1]/text()\u0026#39;)[3] # 3. conference place # CSS selector soup2.select(\u0026#34;li[class = \u0026#39;conference\u0026#39;]\u0026#34;)[3].select(\u0026#34;span\u0026#34;)[1].text # XPath: second \u0026lt;span\u0026gt; element under \u0026lt;li\u0026gt; tree2.xpath(\u0026#39;//li/span[2]/text()\u0026#39;)[3] \r\rlibrary() \r\r\r\r3. SUGI Forum 2018 webpage -\u0026gt; Get paper information \rpython\rR\r\r# create soup3 \u0026amp; tree3 r_sugi_2018 = requests.get(sugi_2018_url, proxies=proxyDict, verify=False) soup3 = BeautifulSoup(r_sugi_2018.text, \u0026#39;lxml\u0026#39;) tree3 = html.fromstring(r_sugi_2018.content) \r\rlibrary() \r\r\r\rgraph LR;\rA[\"div class='paper bl'\"] -- B[\"a id='sgf2018.2659-2018'\"];\rA -- C[\"span class='code'\"];\rA -- D[\"a target='_blank'\" Title \u0026 Link];\rstyle D fill:#f3d7d3;\rA -- E[\"img class='download'\"];\rA -- F[a, Author];\rA -- G[a, Author];\rstyle F fill:#f3d7d3; style G fill:#f3d7d3; A -- H[\"span class='key'\", keyword] --I[b];\rstyle H fill:#f3d7d3; A -- J[\"span class='size'\", Pages] --j[b];\rA -- K[\"span xmlns:gcse='uri:dummy-google-ns' class='size'\", Size] --k[b];\rstyle J fill:#f3d7d3; style K fill:#f3d7d3; \rTitle: Under \u0026lt;a taget=\u0026quot;_blank\u0026quot;\u0026gt; tag \u0026lt;a target=\u0026quot;_blank\u0026quot; href=\u0026quot;https://www.sas.com/content/dam/SAS/support/en/sas-global-forum-proceedings/2018/2695-2018.pdf\u0026quot; \u0026gt;A Case Study of Mining Social Media Data for Disaster Relief: Hurricane Irma\u0026lt;/a\u0026gt;\npython\rR\r\r# CSS selector soup3.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;)[2].text # XPath: find the second \u0026lt;a\u0026gt; element, which is the child of div.paper.wh tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)]/child::a[2]/text()\u0026#39;)[2] \r\rlibrary() \r\r\r\rLink python\rR\r\r# CSS selector soup3.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;)[2].get(\u0026#39;href\u0026#39;) # XPath: tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)]/child::a[2]/@href\u0026#39;)[2] \r\rlibrary() \r\r\r\rAuthor: Under \u0026lt;a\u0026gt; tag\n\u0026lt;a href=\u0026quot;/cgi-bin/xsl_transform.php?x=ag\u0026amp;amp;c=SUGI#bogdidov\u0026quot;\u0026gt;Bogdan Gadidov\u0026lt;/a\u0026gt;\n\u0026lt;a href=\u0026quot;/cgi-bin/xsl_transform.php?x=al\u0026amp;amp;c=SUGI#linhnhle\u0026quot;\u0026gt;Linh Le\u0026lt;/a\u0026gt; python\rR\r\r# beautifulsoup syntax soup3.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[2].find_all(\u0026#39;a\u0026#39;, id=None, target=None) # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find \u0026lt;a\u0026gt; tag without attributes \u0026#34;id\u0026#34; and \u0026#34;target\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/a[not(@id) and not(@target)]/text()\u0026#39;) \r\rlibrary() \r\r\r\rKeyword: Under \u0026lt;span\u0026gt; tag\n\u0026lt;span class=\u0026quot;key\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Keywords:\u0026lt;/b\u0026gt; Text Mining Topic Modeling Time Series \u0026lt;/span\u0026gt; python\rR\r\r# CSS selector soup3.select(\u0026#39;div.paper \u0026gt; span[class = \u0026#34;key\u0026#34;]\u0026#39;)[0].text # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find the \u0026lt;span\u0026gt; tag with attribute class=\u0026#34;key\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/span[@class = \u0026#34;key\u0026#34;]/text()\u0026#39;) \r\rlibrary() \r\r\r\rPages \u0026amp; Size: Under \u0026lt;span\u0026gt; tag \u0026lt;span class=\u0026quot;size\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Pages\u0026lt;/b\u0026gt;:\u0026amp;nbsp;11\u0026amp;nbsp;\u0026lt;/span\u0026gt;\n\u0026lt;span xmlns:gcse=\u0026quot;uri:dummy-google-ns\u0026quot; class=\u0026quot;size\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Size\u0026lt;/b\u0026gt;:\u0026amp;nbsp;660\u0026amp;nbsp;Kb\u0026amp;nbsp;\u0026lt;/span\u0026gt;\npython\rR\r\r# CSS selector soup3.select(\u0026#39;div.paper\u0026#39;)[2].select(\u0026#39;span[class = \u0026#34;size\u0026#34;]\u0026#39;) # XPath: within the \u0026lt;div\u0026gt;, with attribute contains \u0026#34;paper\u0026#34;, find the \u0026lt;span\u0026gt; tag with attribute class=\u0026#34;size\u0026#34; tree3.xpath(\u0026#39;//div[contains(@class, \u0026#34;paper\u0026#34;)][3]/span[@class = \u0026#34;size\u0026#34;]/text()\u0026#39;) \r\rlibrary() \r\r\r\r\r\rGrab paper information (Title, Link, Author, Keyword, Pages, Sizes) from one conference\r\r\rdef one_conf(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#39;div.paper\u0026#39;)) title = [] link = [] author = [] keyword = [] page = [] size = [] for i in range(num_paper): div = soup.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[i] # title \u0026amp; link tl = div.select(\u0026#39;div.paper \u0026gt; a[target = \u0026#34;_blank\u0026#34;]\u0026#39;) if not tl: # tl is empty if div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;code\u0026#34;}) is not None: t_new = div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;code\u0026#34;}).findNextSibling(text=True) else: t_new = div.find(\u0026#34;a\u0026#34;).findNextSibling(text=True) title.append(t_new) link.append(\u0026#34;\u0026#34;) tag = div.select(\u0026#39;div \u0026gt; a\u0026#39;)[1:] ps = [] else: title.append(tl[0].text) link.append(tl[0].get(\u0026#39;href\u0026#39;)) tag = div.select(\u0026#39;div \u0026gt; a\u0026#39;)[2:] ps = div.find_all(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;size\u0026#34;}) # author author2 = [] for j in range(len(tag)): author2.append(tag[j].text) author.append(author2) # keyword key = div.find(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;key\u0026#34;}) if key is None: keyword.append(\u0026#34;\u0026#34;) else: keyword.append(key.text[10:]) # page \u0026amp; size if len(ps) \u0026gt;= 2: p = ps[0] s = ps[1] if p is not None and s is not None: if \u0026#39;Page\u0026#39; in p.text: p2 = re.search(\u0026#39;\\xa0(.*)\\xa0\u0026#39;, p.text) page.append(p2.group(1)) if \u0026#39;Size\u0026#39; in s.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, s.text) s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) else: size.append(\u0026#34;\u0026#34;) elif \u0026#39;Size\u0026#39; in p.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, p.text) s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) page.append(\u0026#34;\u0026#34;) else: size.append(\u0026#34;\u0026#34;) page.append(\u0026#34;\u0026#34;) elif len(ps) == 1: p = ps[0] if \u0026#39;Page\u0026#39; in p.text: p2 = re.search(\u0026#39;\\xa0(.*)\\xa0\u0026#39;, p.text) page.append(p2.group(1)) size.append(\u0026#34;\u0026#34;) elif \u0026#39;Size\u0026#39; in p.text: s2_r = re.search(\u0026#39;\\xa0(.*)\\xa0(.*)\\xa0\u0026#39;, p.text) if s2_r is not None: s2 = s2_r.group(1) + \u0026#34; \u0026#34; + s2_r.group(2) size.append(s2) page.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) else: page.append(\u0026#34;\u0026#34;) size.append(\u0026#34;\u0026#34;) # derive dataframe dataset = pd.DataFrame({\u0026#39;Title\u0026#39; : title, \u0026#39;Keyword\u0026#39;: keyword, \u0026#39;Link\u0026#39; : link, \u0026#39;Author\u0026#39; : author, \u0026#39;Pages\u0026#39; : page, \u0026#39;Size\u0026#39; : size}) dataset[\u0026#39;Author\u0026#39;] = [\u0026#34;, \u0026#34;.join(n) for n in dataset[\u0026#39;Author\u0026#39;]] return dataset \r\r \r\rGrab conference information (Conference place \u0026amp; time \u0026amp; name) from all conferences\r\r\rdef all_paper(url): global conf_name, conf_time, conf_place r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#34;li a\u0026#34;)) df_list = [] for i in range(num_paper): # print(i) conf_name = soup.select(\u0026#34;li a\u0026#34;)[i].text conf_time = soup.select(\u0026#34;li \u0026gt; span\u0026#34;)[2*i].text conf_place = soup.select(\u0026#34;li \u0026gt; span\u0026#34;)[2*i+1].text conf_url = \u0026#39;https://www.lexjansen.com\u0026#39; + soup.select(\u0026#39;li a\u0026#39;)[i].get(\u0026#39;href\u0026#39;)[2:] df = one_conf(conf_url) df.insert(column = \u0026#34;Conference_place\u0026#34;, value = conf_place, loc=0) df.insert(column = \u0026#34;Conference_time\u0026#34;, value = conf_time, loc=0) df.insert(column = \u0026#34;Conference_name\u0026#34;, value = conf_name, loc=0) df_list.append(df) return df_list conf = all_paper(\u0026#39;https://www.lexjansen.com/sugi\u0026#39;) SUGI = pd.concat([pd.DataFrame(conf[x]) for x in range(num_paper)], axis = 0, ignore_index=True) SUGI.to_csv(\u0026#39;SUGI.csv\u0026#39;, index = False) \r\r graph TB;\rA[Title, Link, Author, Keyword, Pages, Size]\rB[Conference name, Conference place, Conference time]\rC[Section name, Best paper flag]\rF[One conference]\rG[One forum]\rA --grab from 468 papers-- F\rF --grab from 46 conferences-- G\rB --merge conference's info --F\rC --merge paper attributes --G\rsubgraph g1 [One paper's info]\rA\rend\rsubgraph g2[Conference information]\rB\rend\rsubgraph g3[Paper attributes]\rC\rend\r\r4. Get paper attributes: Section name Only grab section name \u0026amp; title from the url\ndef title_grab(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) num_paper = len(soup.select(\u0026#39;div.paper\u0026#39;)) title = [] for i in range(num_paper): div = soup.find_all(\u0026#34;div\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;paper\u0026#34;})[i] # print(i) t = div.find(\u0026#34;a\u0026#34;).findNextSibling() if not t: print(\u0026#34;No title\u0026#34;) else: title.append(t.text) title_df = pd.DataFrame({\u0026#39;Title\u0026#39; : title}) return title_df def section_grab(url): s = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(s.text, \u0026#39;lxml\u0026#39;) num_stream = len(soup.select(\u0026#34;div[class=\u0026#39;streams\u0026#39;] span\u0026#34;)) df_list = [] for i in range(num_stream): # print(i) x = soup.find_all(\u0026#34;span\u0026#34;, {\u0026#34;class\u0026#34;: \u0026#34;stream\u0026#34;})[i] sec_name = x.text sec_url = \u0026#39;https://www.lexjansen.com\u0026#39; + x.find(\u0026#34;a\u0026#34;).get(\u0026#39;href\u0026#39;)[2:] df = title_grab(sec_url) df.insert(column = \u0026#34;Section_name\u0026#34;, value = sec_name, loc=0) df_list.append(df) return df_list section = section_grab(\u0026#34;https://www.lexjansen.com/sugi/\u0026#34;) Sections = pd.concat([pd.DataFrame(section[x]) for x in range(num_section)], axis = 0, ignore_index=True) Sections.to_csv(\u0026#39;Sections.csv\u0026#39;, index = False) 5. Merge section names back to the origin file sugi = pd.read_excel(\u0026#39;SUGI.xlsx\u0026#39;) section = pd.read_excel(\u0026#39;Sections.xlsx\u0026#39;) pd.merge(sugi, section, on=\u0026#34;Title\u0026#34;, how=\u0026#39;left\u0026#39;) 6. Get paper attributes: Best paper flag Only grab the title from the url, and provide \u0026ldquo;Y\u0026rdquo; to column \u0026ldquo;Best_paper_fl\u0026rdquo;\ndef best_paper_title(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) title = [] best = soup.select(\u0026#34;div.paperback \u0026gt; a\u0026#34;) if best: for pp in best: title.append(pp.text) paper_df = pd.DataFrame({\u0026#39;Title\u0026#39; : title}) return paper_df def best_paper_fl(url): r_sugi = requests.get(url, proxies=proxyDict, verify=False) soup = BeautifulSoup(r_sugi.text, \u0026#39;lxml\u0026#39;) paper = soup.select(\u0026#34;li a\u0026#34;) df_list = [] for i in range(len(paper)): # print(i) conf_url = \u0026#39;https://www.lexjansen.com\u0026#39; + paper[i].get(\u0026#39;href\u0026#39;)[2:] df = best_paper_title(conf_url) df.insert(column = \u0026#34;Best_paper_fl\u0026#34;, value = \u0026#34;Y\u0026#34;, loc=0) df_list.append(df) return df_list best = best_paper_fl(\u0026#34;https://www.lexjansen.com/sugi/\u0026#34;) best = pd.concat([pd.DataFrame(best[x]) for x in range(46)], axis = 0, ignore_index=True) best.to_csv(\u0026#39;Best_paper.csv\u0026#39;, index = False) 7. Derive final dataset python\rR\r\rsugi = pd.read_excel(\u0026#39;SUGI.xlsx\u0026#39;) section = pd.read_excel(\u0026#39;Sections.xlsx\u0026#39;) best = pd.read_excel(\u0026#39;Best_paper.xlsx\u0026#39;) final = sugi.merge(section, on=\u0026#39;Title\u0026#39;, how=\u0026#39;left\u0026#39;).merge(best, on=\u0026#39;Title\u0026#39;, how=\u0026#39;left\u0026#39;) final.to_csv(\u0026#39;SUGI_paper.csv\u0026#39;, index = False) \r\rlibrary() \r\r\r\r"
},
{
	"uri": "https://rc-web-crawler.github.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Refresh Corner: Web-Crawler This is the webpage for our Web-Crawler study group :) Team members: Aiwen, Haoyong, Mengyue, Shuyan, Songren "
},
{
	"uri": "https://rc-web-crawler.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://rc-web-crawler.github.io/credits/",
	"title": "credits",
	"tags": [],
	"description": "",
	"content": "References: "
},
{
	"uri": "https://rc-web-crawler.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]